{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4100dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import io\n",
    "import re\n",
    "import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65084a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 256\n",
    "WINDOW_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus\n",
    "\n",
    "# corpus = ['In a small village surrounded by mountains, people lived in harmony with nature. The seasons changed gently, bringing new colors and sounds to the valley. In spring, the flowers bloomed, and birds sang joyfully, filling the air with music. Farmers planted crops in neat rows, while children played near the streams, their laughter echoing through the hills. Summer brought warmth, and the fields turned green with life. The village buzzed with activity, as people harvested fruits and vegetables. Autumn arrived with a cool breeze, turning the leaves into shades of gold and red. The village prepared for winter, gathering wood and storing food. Snow covered the land, and the world seemed to slow down. Families gathered around fires, sharing stories of the past and dreaming of the future. Despite the challenges of each season, the villagers remained hopeful and resilient, always finding joy in the simple moments of life.']\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    input_data.lower()\n",
    "    # Create translation table\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # Remove punctuation\n",
    "    return input_data.lower().translate(translator)\n",
    "\n",
    "f = open(\"war_and_peace.txt\", \"r\")\n",
    "input_data = f.read()\n",
    "data = custom_standardization(input_data)\n",
    "corpus = [data]\n",
    " \n",
    "# Convert the corpus to a sequence of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(\"After converting our words in the corpus into vector of integers:\")\n",
    "print(len(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "\n",
    "\n",
    "# Define the parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "# Generate the context-target pairs\n",
    "contexts = []\n",
    "targets = []\n",
    "for sequence in sequences:\n",
    "    for i in range(WINDOW_SIZE, len(sequence) - WINDOW_SIZE):\n",
    "        context = sequence[i - WINDOW_SIZE:i] +\\\n",
    "            sequence[i + 1:i + WINDOW_SIZE + 1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert the contexts and targets to numpy arrays\n",
    "X = np.array(contexts)\n",
    "\n",
    "y = np.zeros((len(X), vocab_size))\n",
    "for i in range(0, len(targets)):\n",
    "    y[i][targets[i]] = 1\n",
    "\n",
    "\n",
    "# Define the CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=EMBEDDING_SIZE,\n",
    "                    input_length=2*WINDOW_SIZE))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "# model.fit(X[:1000], y, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013178f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('cbow_weights.h5')\n",
    "\n",
    "W1 = model.get_weights()[0]\n",
    "W2 = model.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional processing with RELU function\n",
    "\"\"\"def relu(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "weights_new = model.get_weights()[0].copy()\n",
    "threshold = 0.8\n",
    "\n",
    "for i in range(len(weights_new)):\n",
    "    for j in range(len(weights_new[i])):\n",
    "        if relu(weights_new[i][j]) > threshold:\n",
    "            weights_new[i][j] = 1\n",
    "        else:\n",
    "            weights_new[i][j] = 0\n",
    "            \n",
    "index = 35\n",
    "count = 0\n",
    "for i in weights_new[index]:\n",
    "    if i == 1.0:\n",
    "        count += 1\n",
    "\n",
    "print(count)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "def predict_target(context_words):\n",
    "    context_sequence = tokenizer.texts_to_sequences([context_words])[0]\n",
    "    context_sequence = pad_sequences([context_sequence], maxlen=WINDOW_SIZE * 2)\n",
    "    prediction = model.predict(context_sequence)\n",
    "    predicted_word_idx = np.argmax(prediction)\n",
    "    predicted_word = tokenizer.index_word[predicted_word_idx]\n",
    "    return predicted_word\n",
    "\n",
    "# Example prediction\n",
    "sentense = 'rendered the country turbulent and difficult to'\n",
    "context_words = []\n",
    "\n",
    "split_sentense = sentense.split(\" \")\n",
    "\n",
    "if (len(split_sentense) - 1) != WINDOW_SIZE * 2:\n",
    "    print(\"ERROR: sentense does not have the correct lenght for testinng\")\n",
    "else:\n",
    "    for w in split_sentense:\n",
    "        context_words.append(w)\n",
    "    del context_words[WINDOW_SIZE]\n",
    "\n",
    "    predicted_word = predict_target(context_words)\n",
    "    print(f\"Predicted word for context {context_words}: {predicted_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
